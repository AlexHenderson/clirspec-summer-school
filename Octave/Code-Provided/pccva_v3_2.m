function [outcome, dataused]=pcdfa_v3_2(traindata, trainclassnames, trainclassmembership, testdata, testclassnames, pcs, samplingtype, plotresults)

% function: pcdfa (actually PC-CVA)
%
%           Principal Components - Canonical Variates Analysis
%           Principal Components - Discriminant Function Analysis
% version:  3.2
%
% [outcome, dataused]=pcdfa(traindata, trainclassnames, trainclassmembership, testdata, testclassnames, pcs, samplingtype, plotresults);
%
% where:
%   traindata - a collection of spectra in rows used for training the model
%   trainclassnames - a list of the specific class label of each row of the
%                     training set
%   trainclassmembership - a list of the specific class membership of each
%                          row of the training set as numbers
%   testdata - a collection of spectra in rows used for testing the model
%   testclassnames - a list of the specific class label of each row of the
%                    test set
%   pcs - the appropriate number of pcs to use. This can be determined
%           using pressrsstest.m
%   samplingtype - one of 'bootstrap', 'loocv', 'holdout' depending on the
%                  method used to generate the training and test sets
%   plotresults - whether to plot the results; 1=yes, 0=no
%
%   outcome - structure containing the results of the PCA and CVA
%   dataused - structure containing the actual data used for this analysis,
%              basically a copy of the input
%
%   A plot is generated by default unless plotresults==0
%
%   DO NOT USE AUTOSCALED DATA (yet). 
%   THE PROJECTION OF TEST DATA INTO THE CVA-SPACE DOESN'T HANDLE IT.
%
%   Here we perform PCA on the training data and collect the required number
%   of pcs. Then we perform Discriminant Function Analysis (DFA) (actually
%   Canonical Variates Analysis, CVA) using the scores from the PCA. 
%   The test data is then projected into the CV-space and the output saved
%   and optionally plotted. 
%
%   The number of discriminant functions (canonical variates) is the number
%   of groups less 1. 
%
%   Copyright (c) April 2012, Alex Henderson 
%   Contact email: alex.henderson@manchester.ac.uk
%   Licenced under the GNU General Public License (GPL) version 3
%   http://www.gnu.org/copyleft/gpl.html
%   Other licensing options are available, please contact Alex for details
%   If you use this file in your work, please acknowledge the author(s) in
%   your publications. 

%   version 1.0 April 2012, Alex Henderson. 
%   Initial release. 
%   version 2.0 July 2012,  Alex Henderson. 
%   Modified to handle situations where there are only two classes and the
%   plotting function went wrong. 
%   version 3.0 July 2012, Alex Henderson. 
%   Changed the histogram limits although the main difference in plotting
%   is the way MATLAB decides to draw the axes. 
%   version 3.1 September 2013, Alex Henderson. 
%   Fixed error when projecting in a single datapoint in DFA (eg when doing
%   LOOCV).
%   version 3.2 September 2013, Alex Henderson. 
%   Fixed colour of projected data for single point histograms. This is the
%   case for LOOCV and 2 group CVA. 

[numtrain,colstrain] = size(traindata);
[numtest, colstest] = size(testdata);

numtrainclasses=size(unique(trainclassnames, 'rows'), 1);
numtestclasses=size(unique(testclassnames, 'rows'), 1);

if ((strcmpi(samplingtype,'bootstrap')||strcmpi(samplingtype,'holdout')) && (numtrainclasses ~= numtestclasses))
    % only an error for bootstrapping, loocv is OK
    % holdout and kfold are likely to be sorted out by the stratification routine
    error('Mismatch in number of classes in training and test sets. Something is underrepresented.');
end

dfs=numtrainclasses -1;

% Sort the data (and associated class membership and labels) so that the
% projected data keeps the same class order and therefore the same colours
% in the output plots. 

% First sort the class names
% Append the new sort order to the old sort order
% Re-sort the indicies back so we have the old sort order
% Append this index matrix to the other data sets (data, class membership
% etc) and sort on the new sort order column
% Remove the appended index matrix columns
% We could do this much easier now we have the class membership list as a
% vector of doubles. This scheme was designed for having class names as a
% list of strings which couldn't be appended to the data matrix (doubles)

% [trainclassnames,trainclassnamesidx]=sortrows(trainclassnames);
% trainclassnamesidx = [trainclassnamesidx, (1:numtrain)'];
% trainclassnamesidx=sortrows(trainclassnamesidx, 1);
% traindata=[traindata,trainclassnamesidx];
% traindata=sortrows(traindata, colstrain+2);
% traindata=traindata(:,1:colstrain);
% trainclassmembership=[trainclassmembership,trainclassnamesidx];
% trainclassmembership=sortrows(trainclassmembership, 3);
% trainclassmembership=trainclassmembership(:,1);
% 
% 
% [testclassnames,testclassnamesidx]=sortrows(testclassnames);
% testclassnamesidx = [testclassnamesidx, (1:numtest)'];
% testclassnamesidx=sortrows(testclassnamesidx, 1);
% testdata=[testdata,testclassnamesidx];
% testdata=sortrows(testdata, colstest+2);
% testdata=testdata(:,1:colstest);

% mean center
trainingmean=mean(traindata);
traindata=traindata-repmat(trainingmean, numtrain, 1);
% remove the TRAINING mean from the TEST data
testdata=testdata-repmat(trainingmean, numtest, 1);

% do the maths
%[pcscores, pcloadings, pcpercent_explained] = alex_pca(traindata, pcs, 'nipals');
[pcloadings, pcscores, pcvariances] = princomp(traindata, 'econ');
    pcloadings=pcloadings(:,1:pcs)';
    pcscores=pcscores(:,1:pcs);
    pcpercent_explained = 100*pcvariances/sum(pcvariances);
    pcpercent_explained=pcpercent_explained(1:pcs,:)';

% here dfa is really cva    
[dfscores, dfloadings, dfeigenvals] = dfa(pcscores, trainclassmembership, dfs);

% locate the test data in the new matrix rotation
projtest=testdata * pcloadings';
dprojtest=projtest * dfloadings * diag(dfeigenvals);

% store for output
outcome.pcascores=pcscores;
outcome.pcaloadings=pcloadings;
outcome.pcapercent_explained=pcpercent_explained;
outcome.pcatestvars=projtest;

outcome.dfascores=dfscores;
outcome.dfaloadings=dfloadings;
outcome.dfaeigenvals=dfeigenvals;
outcome.dfatestvars=dprojtest;
outcome.dfs=dfs;

dataused.traindata = traindata;
dataused.trainmean = trainingmean;
dataused.trainclassnames = trainclassnames;
dataused.trainclassmembership = trainclassmembership;
dataused.testdata = testdata;
dataused.testclassnames = testclassnames;
dataused.pcs = pcs;

% Now plot the results

if exist('plotresults', 'var') && plotresults == 0
    % don't plot anything
    % this means the plots are done by default
else

    colours='bgrcmky';
    coloursasmap = [0,0,1; 0,1,0; 1,0,0; 0,1,1; 1,0,1; 1,1,0; 0,0,0];
    testcolours=colours;
    if (numtrainclasses~=numtestclasses)
        untrainnames=unique(trainclassnames, 'rows');
        untestnames=unique(testclassnames, 'rows');
        testcolours = colours(ismember(untrainnames,untestnames, 'rows'));
    end
    axiscolour='k';
    figure;

    %pca of training data
    subplot(2,2,1);
    gscatter(pcscores(:,1), pcscores(:,2), trainclassnames, colours, 'o'); 
    xlabel(['principal component 1 (', num2str(pcpercent_explained(1),4), '%)']);
    ylabel(['principal component 2 (', num2str(pcpercent_explained(2),4), '%)']);
    title('pca training data');
    legend('Location','Best');
    
    hold on;
    limits=axis;
    xmin = limits(1,1);
    xmax = limits(1,2);
    ymin = limits(1,3);
    ymax = limits(1,4);
    plot([0,0], [0,ymax], axiscolour);
    plot([0,0], [0,ymin], axiscolour);
    plot([0,xmax], [0,0], axiscolour);
    plot([0,xmin], [0,0], axiscolour);
    hold off;

    %pca of training and test data
    subplot(2,2,2);
    gscatter(pcscores(:,1), pcscores(:,2), trainclassnames, colours, 'o');
    hold on;
    gscatter(projtest(:,1), projtest(:,2), testclassnames, testcolours, '.'); 
    hold off;
    xlabel(['principal component 1 (', num2str(pcpercent_explained(1),4), '%)']);
    ylabel(['principal component 2 (', num2str(pcpercent_explained(2),4), '%)']);
    title('pca training and test data');
    legend('Location','Best');
    hold on;
    limits=axis;
    xmin = limits(1,1);
    xmax = limits(1,2);
    ymin = limits(1,3);
    ymax = limits(1,4);
    plot([0,0], [0,ymax], axiscolour);
    plot([0,0], [0,ymin], axiscolour);
    plot([0,xmax], [0,0], axiscolour);
    plot([0,xmin], [0,0], axiscolour);
    hold off;


    % dfa on its own
    subplot(2,2,3);
    if (dfs==1) % therefore two classes
        untrainnames=unique(trainclassnames, 'rows');
        trainindex1=find(trainclassnames==(untrainnames(1)));
        trainindex2=find(trainclassnames==(untrainnames(2)));
        
%        absmax=max(abs(max(dfscores)),abs(min(dfscores)));
        % add a tiny amount to the upper limit for edges so that the
        % largest value will fall inside the limit rather than creating a
        % large bin to be just inside. Not really sure if this makes any
        % difference since MATLAB is 'prettifying' the output anyway
%        edges=linspace(-absmax, absmax+0.00000001,length(dfscores));

        minval = min(dfscores);
        maxval = max(dfscores);
        extrabucket= (maxval-minval)/length(dfscores);
        edges=linspace(minval, maxval+extrabucket,length(dfscores));
        
        [n1,bin1] = histc(dfscores(trainindex1,1),edges);
        [n2,bin2] = histc(dfscores(trainindex2,1),edges);
        n=[n1,n2];
        bar(edges,n,2,'histc');  
        histcolourmap=coloursasmap(1:2, :);
        colormap(histcolourmap);

        xlabel(['canonical variate 1 (eig=', num2str(dfeigenvals(1),4), ')']);
        ylabel('counts per bin');
        title(['cva training data (', num2str(pcs), ' PCs)']);
        legend(untrainnames(1,:), untrainnames(2,:),'Location','Best');
    else
        gscatter(dfscores(:,1), dfscores(:,2), trainclassnames, colours, 'o'); 
        xlabel(['canonical variate 1 (eig=', num2str(dfeigenvals(1),4), ')']);
        ylabel(['canonical variate 2 (eig=', num2str(dfeigenvals(2),4), ')']);
        title(['cva training data (', num2str(pcs), ' PCs)']);
        legend('Location','Best');
    end
    hold on;
    limits=axis;
    xmin = limits(1,1);
    xmax = limits(1,2);
    ymin = limits(1,3);
    ymax = limits(1,4);
    plot([0,0], [0,ymax], axiscolour);
    plot([0,0], [0,ymin], axiscolour);
    plot([0,xmax], [0,0], axiscolour);
    plot([0,xmin], [0,0], axiscolour);
    hold off;

    % dfa and test data
    subplot(2,2,4);
    if (dfs==1)
        % calculate limits for histogram overlay
%         absmaxtrain=max(abs(max(dfscores)),abs(min(dfscores)));
%         absmaxtest=max(abs(max(dprojtest)),abs(min(dprojtest)));
%         
%         absmax=max(absmaxtrain,absmaxtest);
%         maxedges=max(length(dfscores),length(dprojtest));
%         
%         edges=linspace(-absmax,absmax,maxedges);

        % put the training and test scores together and generate an axis
        % from the lot
        minval = min([dfscores;dprojtest]);
        maxval = max([dfscores;dprojtest]);
        extrabucket= (maxval-minval)/length(dfscores);
        edges=linspace(minval, maxval+extrabucket,length(dfscores));
        
        % training data
        untrainnames=unique(trainclassnames, 'rows');
        trainindex1=find(trainclassnames==(untrainnames(1)));
        if(size(untrainnames,1) > 1)
            trainindex2=find(trainclassnames==(untrainnames(2)));
        end
        
        [n1,bin1] = histc(dfscores(trainindex1,1),edges);
        [n2,bin2] = histc(dfscores(trainindex2,1),edges);
        n=[n1,n2];
        bar(edges,n,2,'histc');  
        histcolourmap=coloursasmap(1:2, :);
        colormap(histcolourmap);

        hold on;

        % test data
        untestnames=unique(testclassnames, 'rows');
        testindex1=find(testclassnames==(untestnames(1)));
        if(size(untestnames,1) > 1)
            % For LOOCV we don't have a second test data point
            testindex2=find(testclassnames==(untestnames(2)));
        end
        
        [n1,bin1] = histc(dprojtest(testindex1,1),edges);
        n=n1;
        if(size(untestnames,1) > 1)
            % For LOOCV we don't have a second test data point
            [n2,bin2] = histc(dprojtest(testindex2,1),edges);
            n=[n1,n2];
        end
        n=-n;
        
        if(size(untestnames,1) > 1)
            bar(edges,n,2,'histc');  
            histcolourmap=coloursasmap(1:2, :);
            colormap(histcolourmap);
        else
            % When we only have a single point, we need to specify the
            % colour of the bar explicitly. 
            testcharthandle=bar(edges,n,2,'histc');  
            set(testcharthandle,'FaceColor',testcolours);
        end
        xlabel(['canonical variate 1 (eig=', num2str(dfeigenvals(1),4), ')']);
        ylabel('counts per bin (neg = test set)');
        title(['cva training and test data (', num2str(pcs), ' PCs)']);
        legend(untrainnames(1,:), untrainnames(2,:),'Location','Best');
        
        hold off;
    else
        gscatter(dfscores(:,1), dfscores(:,2), trainclassnames, colours, 'o');
        hold on;
        gscatter(dprojtest(:,1), dprojtest(:,2), testclassnames, testcolours, '.');
        hold off;
        xlabel(['canonical variate 1 (eig=', num2str(dfeigenvals(1),4), ')']);
        ylabel(['canonical variate 2 (eig=', num2str(dfeigenvals(2),4), ')']);
        title(['cva training and test data (', num2str(pcs), ' PCs)']);
        legend('Location','Best');
    end
    hold on;
    limits=axis;
    xmin = limits(1,1);
    xmax = limits(1,2);
    ymin = limits(1,3);
    ymax = limits(1,4);
    plot([0,0], [0,ymax], axiscolour);
    plot([0,0], [0,ymin], axiscolour);
    plot([0,xmax], [0,0], axiscolour);
    plot([0,xmin], [0,0], axiscolour);
    hold off;
    
end
